{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0357cc3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Notebook Summary: Exploring \"DiffEdit\" for Mask Generation & Image Editing##\n",
    "\n",
    "This notebook investigates the [\"DiffEdit\"](https://arxiv.org/abs/2210.11427) technique for automatically generating editing masks. The method involves:\n",
    "1.  Providing an initial image (as latents).\n",
    "2.  Using a \"source prompt\" (describing the original object) and a \"target prompt\" (describing the desired change).\n",
    "3.  Generating multiple noisy versions with both prompts and comparing the predicted noise to isolate differences, forming a mask.\n",
    "4.  This mask is then used to guide image generation with the standard Stable Diffusion model (in a custom img2img-like process) and compared against a dedicated inpainting model.\n",
    "\n",
    "**Key Conclusions:**\n",
    "*   **DiffEdit's Effectiveness:** The mask generation works reasonably well *only if* the initial image was itself generated by the Stable Diffusion model (e.g., the dog example where we know the generation prompt and seed). It struggles to create useful masks for arbitrary external photos.\n",
    "*   **Prompt Sensitivity:** It's difficult to craft source and target prompts that reliably produce a precise and useful mask.\n",
    "*   **Generalization Issues:** The mask generation technique doesn't generalize well beyond images where the model has a strong prior (i.e., images it can generate well on its own, or for which the original generation parameters are known).\n",
    "*   **Inpainting Models are Superior:** Dedicated inpainting models (like `StableDiffusionInpaintPipeline`) are significantly more effective and produce much better results for the actual editing task compared to using the DiffEdit-generated mask with the standard Stable Diffusion pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel, PNDMScheduler\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "if \"mps\" == torch_device: os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space.\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270368b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_latent(tensor):\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tensor.to(torch_device)*2-1) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32353989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    scheduler.timesteps = scheduler.timesteps.to(torch.float32) # minor fix to ensure MPS compatibility, fixed in diffusers PR 3925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image with PIL\n",
    "horse_img = Image.open('horse.png').resize((512,512))\n",
    "horse_img = horse_img.convert('RGB')\n",
    "init_image = tfms.ToTensor()(horse_img)\n",
    "init_latents = pil_to_latent(horse_img)\n",
    "horse_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d444023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the four channels of this latent representation:\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):\n",
    "    axs[c].imshow(init_latents[0][c].cpu(), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_embd(prompt):\n",
    "\ttext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\twith torch.no_grad():\n",
    "\t\ttext_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\treturn text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(prompt, start_step, init_latents,  num_inference_steps = 50, guidance_scale = 8, seed = 52, progress_bar = False):\n",
    "\theight = 512                        # default height of Stable Diffusion\n",
    "\twidth = 512                         # default width of Stable Diffusion\n",
    "\tgenerator = torch.manual_seed(seed)   # Seed generator to create the inital latent noise\n",
    "\tbatch_size = 1\n",
    "\n",
    "\tprompt_emedding = prompt_to_embd(prompt)\n",
    "\tuncond_embeddings = prompt_to_embd(\"\")\n",
    "\ttext_embeddings = torch.cat([uncond_embeddings, prompt_emedding])\n",
    "\n",
    "\t# Prep Scheduler (setting the number of inference steps)\n",
    "\tset_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "\t# Prep latents (noising appropriately for start_step)\n",
    "\tnoise = torch.randn_like(init_latents)\n",
    "\tlatents = scheduler.add_noise(init_latents, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "\tlatents = latents.to(torch_device).float()\n",
    "\n",
    "\n",
    "\t# Loop\n",
    "\tfor i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps), disable=not progress_bar):\n",
    "\t\tif i >= start_step: # << This is the only modification to the loop we do\n",
    "\n",
    "\t\t\t# expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "\t\t\tlatent_model_input = torch.cat([latents] * 2)\n",
    "\t\t\tsigma = scheduler.sigmas[i]\n",
    "\t\t\tlatent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "\t\t\t# predict the noise residual\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tnoise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "\t\t\t# perform guidance\n",
    "\t\t\tnoise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "\t\t\tnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "\t\t\t# compute the previous noisy sample x_t -> x_t-1\n",
    "\t\t\tlatents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "\treturn latents, noise_pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, n1 = generate_img('horse on the green grass', 25, init_latents=init_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850056b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(prompt_src, prompt_dest, init_latents, start_step=25, niter=10):\n",
    "\tdiffs = []\n",
    "\ttorch.manual_seed(torch.seed())\n",
    "\tseeds = list(range(10))\n",
    "\tfor i in tqdm(range(niter)):\n",
    "\t\tim1, n1 = generate_img(prompt_src, start_step, init_latents=init_latents, seed=seeds[i], progress_bar=False)\t\n",
    "\t\tim2, n2 = generate_img(prompt_dest, start_step, init_latents=init_latents,seed=seeds[i], progress_bar=False)\n",
    "\t\tdiffs.append((n1-n2).norm(p=2, dim=1).detach().cpu())\n",
    "\t\t\n",
    "\tmean_diff = torch.cat(diffs).mean(dim=0)\n",
    "\tnormalized_diff = (mean_diff - mean_diff.min())/(mean_diff.max()-mean_diff.min())\n",
    "\tmask = (normalized_diff > 0.5).type(torch.float32)\n",
    "\treturn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0aac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_mask('horse standing on the grass', 'zebra standing on the grass', init_latents=init_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray((mask*255.).cpu().numpy().astype(np.uint8)).resize((512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask(mask):\n",
    "\tsmooth_mask = F.max_pool2d(mask[None, None], kernel_size=5, stride=1, padding=2)\n",
    "\tsmooth_mask = smooth_mask.squeeze()\n",
    "\timg_mask = Image.fromarray((smooth_mask*255.).cpu().numpy().astype(np.uint8))\n",
    "\treturn smooth_mask, img_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, img_mask = smooth_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask.resize((512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.blend(horse_img.convert(\"RGBA\"), img_mask.resize((512,512)).convert(\"RGBA\"), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24725eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified, noise = generate_img('huge wolf standing on the grass', 30, init_latents,mask=mask.to(torch_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_to_pil(modified)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cd05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d68fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pipe('huge wolf standing on the grass', horse_img, F.interpolate(mask[None, None], (512,512)), strength=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba75857",
   "metadata": {},
   "outputs": [],
   "source": [
    "img['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "im, lat = generate_img('big golden retriver in the middle center of the grass ', 0, init_latents, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_img = latents_to_pil(im)[0]\n",
    "init_image = tfms.ToTensor()(dog_img)\n",
    "init_latents = pil_to_latent(dog_img)\n",
    "dog_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30468be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1, lat = generate_img('big border collie in the center of the grass ', 0, init_latents, seed=1)\n",
    "latents_to_pil(im1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_mask = create_mask('big golden retriver in the center of the grass ', 'big border collie in the center of the grass ',init_latents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ebaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf045be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_mask, dog_mask_img = smooth_mask(dog_mask)\n",
    "dog_mask_img.resize((512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a92139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.blend(dog_img.convert('RGBA'), dog_mask_img.resize((512,512)).convert('RGBA'), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modified, noise = generate_img('big border collie in the center of the grass ', 15, init_latents,mask=dog_mask.to(torch_device), seed=0)\n",
    "latents_to_pil(modified)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified, noise = generate_img('big  Cavalier King Charles Spaniel in the center of the grass ', 15, init_latents,mask=dog_mask.to(torch_device), seed=1)\n",
    "latents_to_pil(modified)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ffa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified, noise = generate_img('big cyclist in the center of the grass ', 5, init_latents,mask=dog_mask.to(torch_device), seed=1)\n",
    "latents_to_pil(modified)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793affa",
   "metadata": {},
   "source": [
    "# Pretrained for inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pipe('big Australian Shepherd in the center of the grass ', dog_img, F.interpolate(dog_mask[None, None], (512,512)), strength=1, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "img['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pipe('big scary yeti in the center of the grass ', dog_img, F.interpolate(dog_mask[None, None], (512,512)), strength=1, seed=1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pipe('big cyclist in the center of the grass ', dog_img, F.interpolate(dog_mask[None, None], (512,512)), strength=1, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "img['images'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
